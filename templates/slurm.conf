# slurm.conf file generated by configurator easy.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
ControlMachine={{ ansible_hostname }}
#ControlAddr=
# 
MailProg=/usr/sbin/sendmail
MpiDefault=none
#MpiParams=ports=#-# 
ProctrackType=proctrack/pgid
ReturnToService=1
SlurmctldPidFile={{ slurm_pid_dir }}/slurmctld.pid
#SlurmctldPort=6817 
SlurmdPidFile={{ slurm_pid_dir }}/slurmd.pid
#SlurmdPort=6818 
SlurmdSpoolDir=/var/lib/slurm-llnl/slurmd
SlurmUser={{ slurm_user }}
#SlurmdUser=root 
StateSaveLocation=/var/lib/slurm-llnl/slurmctld
SwitchType=switch/none
TaskPlugin=task/affinity
TaskPluginParam=Sched
# 
# 
# TIMERS 
#KillWait=30 
#MinJobAge=300 
#SlurmctldTimeout=120 
#SlurmdTimeout=300 
# 
# 
# SCHEDULING 
FastSchedule=1
SchedulerType=sched/backfill
#SchedulerPort=7321 
# cons_res: schedule individual cores
SelectType=select/cons_res
SelectTypeParameters=CR_Core
# this ensures submissions fail if they ask for more resources than available on the partition 
EnforcePartLimits=ALL
#
# 
# LOGGING AND ACCOUNTING 
AccountingStorageType=accounting_storage/none
ClusterName=cluster
#JobAcctGatherFrequency=30 
JobAcctGatherType=jobacct_gather/none
#SlurmctldDebug=3 
SlurmctldLogFile=/var/log/slurm-llnl/slurmctld.log
#SlurmdDebug=3 
SlurmdLogFile=/var/log/slurm-llnl/slurmd.log
# 
# 
# BEGIN ANSIBLE MANAGED NODES
NodeName={{ ansible_hostname }} Sockets={{ ansible_processor_count }} CoresPerSocket={{ ansible_processor_cores }} ThreadsPerCore={{ ansible_processor_threads_per_core }} State=UNKNOWN
PartitionName={{ slurm_partition_name }} Nodes={{ ansible_hostname }} Default=YES MaxTime=INFINITE State=UP MaxNodes=1 MaxCPUsPerNode={{ ansible_processor_vcpus }}
# END ANSIBLE MANAGED NODES
